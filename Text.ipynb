{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUVzpo2wWGX5"
      },
      "source": [
        "# ðŸ§¬ Two-Stage Microbiome Disease Classification Pipeline\n",
        "\n",
        "## ðŸŽ¯ Pipeline Overview\n",
        "\n",
        "This notebook implements a sophisticated two-stage classification system:\n",
        "\n",
        "**Stage 1:** Binary Classification (Healthy vs Diseased)\n",
        "- Distinguishes between healthy and diseased samples\n",
        "- Outputs probability of disease\n",
        "\n",
        "**Stage 2:** Multi-Class Disease Classification\n",
        "- Predicts specific disease type for diseased samples\n",
        "- Computes risk scores across all disease classes\n",
        "\n",
        "**Final Output:** Combined risk assessment with probabilities for all classes including Healthy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgIJnLbRWGX7"
      },
      "source": [
        "## ðŸ“¦ Cell 1: Install and Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E80K_-JuWGX8",
        "outputId": "a8502247-f9f7-41d0-e632-dd6462e76203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive (for Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUPRgGVpWGX9",
        "outputId": "9cb4981a-93cc-4d7a-bcf4-f67254ecd072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ–¥ï¸  Using device: cuda\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "# Scikit-learn imports\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report,\n",
        "    roc_auc_score, roc_curve, auc,\n",
        "    precision_recall_fscore_support,\n",
        "    accuracy_score, f1_score\n",
        ")\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nðŸ–¥ï¸  Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufR848hzWGX9"
      },
      "source": [
        "## ðŸ“‚ Cell 2: Configuration and File Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JtAwtTRtWGX-"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    # File paths\n",
        "    'healthy_file': '/content/drive/MyDrive/Gut Sean/disease_data/Healthy.csv',\n",
        "    'disease_dir': '/content/drive/MyDrive/Gut Sean/combined_csv_files/',\n",
        "    'output_dir': '/content/drive/MyDrive/Gut Sean/outputs/',\n",
        "\n",
        "    # Preprocessing\n",
        "    'log_transform': True,\n",
        "    'log_constant': 1e-6,\n",
        "    'min_variance_threshold': 1e-10,\n",
        "    'drop_columns': ['Run_ID', 'SampleID', 'Sample_ID', 'ID'],\n",
        "\n",
        "    # Feature selection\n",
        "    'n_features': 500,\n",
        "    'feature_selection_method': 'f_classif',\n",
        "\n",
        "    # Train/val/test split\n",
        "    'test_size': 0.15,\n",
        "    'val_size': 0.15,\n",
        "    'random_state': 42,\n",
        "\n",
        "    # ========== UPDATED: Class imbalance handling ==========\n",
        "    'use_weighted_sampler': False,      # CHANGED: Disable weighted sampler\n",
        "    'undersample_healthy': True,\n",
        "    'undersample_ratio': 1.0,            # CHANGED: 1:1 ratio instead of 0.5:1\n",
        "    'use_class_weights': True,           # Keep weighted loss\n",
        "\n",
        "    # ========== UPDATED: Model architecture ==========\n",
        "    'hidden_dims': [1024, 512, 256],    # CHANGED: Wider network\n",
        "    'dropout': 0.3,\n",
        "    'use_batch_norm': True,\n",
        "\n",
        "    # ========== UPDATED: Training parameters ==========\n",
        "    'batch_size': 64,                    # CHANGED: Larger batches\n",
        "    'learning_rate': 0.001,\n",
        "    'weight_decay': 5e-5,                # CHANGED: Reduced weight decay\n",
        "    'num_epochs': 100,\n",
        "    'patience': 15,                      # CHANGED: More patience\n",
        "\n",
        "    # ========== NEW: Learning rate scheduling ==========\n",
        "    'use_lr_scheduler': True,\n",
        "    'scheduler_type': 'cosine',          # 'cosine' or 'plateau'\n",
        "    'lr_warmup_epochs': 5,\n",
        "\n",
        "    # ========== UPDATED: Stage 2 threshold ==========\n",
        "    'disease_threshold_low': 0.3,        # NEW: Soft thresholding\n",
        "    'disease_threshold_high': 0.7,       # NEW: Soft thresholding\n",
        "\n",
        "    # ========== NEW: Focal loss parameters ==========\n",
        "    'use_focal_loss': True,\n",
        "    'focal_alpha': 0.25,\n",
        "    'focal_gamma': 2.0,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3szGUKKwWGX-"
      },
      "source": [
        "## ðŸ”§ Cell 3: Data Loading Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhvQlJueWGX-",
        "outputId": "74e1bce1-e07a-4e6c-e52c-fc4c4196267f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š Loading data files...\n",
            "============================================================\n",
            "âœ“ Healthy: 1320693 samples, 5 features\n",
            "âœ“ COVID19: 119774 lines, 5 features\n",
            "âœ“ Colorectal_Neoplasms: 93665 lines, 5 features\n",
            "âœ“ Crohn_s: 97457 lines, 5 features\n",
            "âœ“ Diabetes: 46962 lines, 5 features\n",
            "âœ“ IBS: 22118 lines, 5 features\n",
            "âœ“ KidneyFailure: 43111 lines, 5 features\n",
            "âœ“ Parkinsons: 67049 lines, 5 features\n",
            "âœ“ Ulcerative_Colitis: 64838 lines, 5 features\n",
            "âœ“ cysticfibrosis: 31888 lines, 5 features\n",
            "âœ“ nafld: 52297 lines, 5 features\n",
            "============================================================\n",
            "\n",
            "âœ… Loaded 11 datasets (11 classes)\n"
          ]
        }
      ],
      "source": [
        "def load_all_data(healthy_file, disease_dir, verbose=True):\n",
        "    \"\"\"\n",
        "    Load Healthy.csv and all disease CSV files dynamically.\n",
        "\n",
        "    Returns:\n",
        "        all_data: List of (dataframe, label) tuples\n",
        "        disease_names: List of disease names\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "    disease_names = ['Healthy']\n",
        "\n",
        "    # Load Healthy samples\n",
        "    if verbose:\n",
        "        print(\"\\nðŸ“Š Loading data files...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        df_healthy = pd.read_csv(healthy_file)\n",
        "        all_data.append((df_healthy, 'Healthy'))\n",
        "        if verbose:\n",
        "            print(f\"âœ“ Healthy: {len(df_healthy)} samples, {len(df_healthy.columns)} features\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading Healthy.csv: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    # Load disease files\n",
        "    disease_files = glob.glob(os.path.join(disease_dir, '*.csv'))\n",
        "\n",
        "    if not disease_files:\n",
        "        print(f\"âš ï¸  No disease CSV files found in {disease_dir}\")\n",
        "        return None, None\n",
        "\n",
        "    for file_path in sorted(disease_files):\n",
        "        disease_name = Path(file_path).stem\n",
        "\n",
        "        try:\n",
        "            df_disease = pd.read_csv(file_path)\n",
        "            all_data.append((df_disease, disease_name))\n",
        "            disease_names.append(disease_name)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"âœ“ {disease_name}: {len(df_disease)} lines, {len(df_disease.columns)} features\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸  Error loading {disease_name}: {e}\")\n",
        "\n",
        "    if verbose:\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"\\nâœ… Loaded {len(all_data)} datasets ({len(disease_names)} classes)\")\n",
        "\n",
        "    return all_data, disease_names\n",
        "\n",
        "\n",
        "# Load data\n",
        "all_data, disease_names = load_all_data(\n",
        "    CONFIG['healthy_file'],\n",
        "    CONFIG['disease_dir']\n",
        ")\n",
        "\n",
        "if all_data is None:\n",
        "    raise ValueError(\"Failed to load data. Please check file paths.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjuMdp12aRma",
        "outputId": "c407c325-66b2-40f7-ca54-06a7eb77d1b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”„ Pivoting data from long to wide format...\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š Processing Healthy:\n",
            "   Original shape: (1320693, 5)\n",
            "   Columns: ['run_id', 'ncbi_taxon_id', 'relative_abundance', 'taxon_rank_level', 'scientific_name']\n",
            "   Unique samples (run_id): 12236\n",
            "   Unique taxa (ncbi_taxon_id): 2179\n",
            "   âœ“ Pivoted shape: (12236, 2180)\n",
            "   âœ“ Samples: 12236, Features: 2179\n",
            "\n",
            "ðŸ“Š Processing COVID19:\n",
            "   Original shape: (119774, 5)\n",
            "   Columns: ['run_id', 'ncbi_taxon_id', 'relative_abundance', 'taxon_rank_level', 'scientific_name']\n",
            "   Unique samples (run_id): 1813\n",
            "   Unique taxa (ncbi_taxon_id): 1273\n",
            "   âœ“ Pivoted shape: (1813, 1274)\n",
            "   âœ“ Samples: 1813, Features: 1273\n",
            "\n",
            "ðŸ“Š Processing Colorectal_Neoplasms:\n",
            "   Original shape: (93665, 5)\n",
            "   Columns: ['run_id', 'ncbi_taxon_id', 'relative_abundance', 'taxon_rank_level', 'scientific_name']\n",
            "   Unique samples (run_id): 793\n",
            "   Unique taxa (ncbi_taxon_id): 1377\n",
            "   âœ“ Pivoted shape: (793, 1378)\n",
            "   âœ“ Samples: 793, Features: 1377\n",
            "\n",
            "ðŸ“Š Processing Crohn_s:\n",
            "   Original shape: (97457, 5)\n",
            "   Columns: ['run_id', 'ncbi_taxon_id', 'relative_abundance', 'taxon_rank_level', 'scientific_name']\n",
            "   Unique samples (run_id): 1425\n",
            "   Unique taxa (ncbi_taxon_id): 1184\n",
            "   âœ“ Pivoted shape: (1425, 1185)\n",
            "   âœ“ Samples: 1425, Features: 1184\n",
            "\n",
            "ðŸ“Š Processing Diabetes:\n",
            "   Original shape: (46962, 5)\n",
            "   Columns: ['run_id', 'ncbi_taxon_id', 'relative_abundance', 'taxon_rank_level', 'scientific_name']\n",
            "   Unique samples (run_id): 326\n",
            "   Unique taxa (ncbi_taxon_id): 1053\n",
            "   âœ“ Pivoted shape: (326, 1054)\n",
            "   âœ“ Samples: 326, Features: 1053\n",
            "\n",
            "ðŸ“Š Processing IBS:\n",
            "   Original shape: (22118, 5)\n",
            "   Columns: ['run_id', 'ncbi_taxon_id', 'relative_abundance', 'taxon_rank_level', 'scientific_name']\n",
            "   Unique samples (run_id): 379\n",
            "   Unique taxa (ncbi_taxon_id): 500\n",
            "   âœ“ Pivoted shape: (379, 501)\n",
            "   âœ“ Samples: 379, Features: 500\n",
            "\n",
            "ðŸ“Š Processing KidneyFailure:\n",
            "   Original shape: (43111, 5)\n",
            "   Columns: ['run_id', 'ncbi_taxon_id', 'relative_abundance', 'taxon_rank_level', 'scientific_name']\n",
            "   Unique samples (run_id): 347\n",
            "   Unique taxa (ncbi_taxon_id): 896\n",
            "   âœ“ Pivoted shape: (347, 897)\n",
            "   âœ“ Samples: 347, Features: 896\n",
            "\n",
            "ðŸ“Š Processing Parkinsons:\n",
            "   Original shape: (67049, 5)\n",
            "   Columns: ['run_id', 'ncbi_taxon_id', 'relative_abundance', 'taxon_rank_level', 'scientific_name']\n",
            "   Unique samples (run_id): 549\n",
            "   Unique taxa (ncbi_taxon_id): 1053\n",
            "   âœ“ Pivoted shape: (549, 1054)\n",
            "   âœ“ Samples: 549, Features: 1053\n",
            "\n",
            "ðŸ“Š Processing Ulcerative_Colitis:\n",
            "   Original shape: (64838, 5)\n",
            "   Columns: ['run_id', 'ncbi_taxon_id', 'relative_abundance', 'taxon_rank_level', 'scientific_name']\n",
            "   Unique samples (run_id): 859\n",
            "   Unique taxa (ncbi_taxon_id): 1055\n",
            "   âœ“ Pivoted shape: (859, 1056)\n",
            "   âœ“ Samples: 859, Features: 1055\n",
            "\n",
            "ðŸ“Š Processing cysticfibrosis:\n",
            "   Original shape: (31888, 5)\n",
            "   Columns: ['run_id', 'ncbi_taxon_id', 'relative_abundance', 'taxon_rank_level', 'scientific_name']\n",
            "   Unique samples (run_id): 488\n",
            "   Unique taxa (ncbi_taxon_id): 953\n",
            "   âœ“ Pivoted shape: (488, 954)\n",
            "   âœ“ Samples: 488, Features: 953\n",
            "\n",
            "ðŸ“Š Processing nafld:\n",
            "   Original shape: (52297, 5)\n",
            "   Columns: ['run_id', 'ncbi_taxon_id', 'relative_abundance', 'taxon_rank_level', 'scientific_name']\n",
            "   Unique samples (run_id): 493\n",
            "   Unique taxa (ncbi_taxon_id): 944\n",
            "   âœ“ Pivoted shape: (493, 945)\n",
            "   âœ“ Samples: 493, Features: 944\n",
            "\n",
            "============================================================\n",
            "âœ… Pivoting complete!\n",
            "   Processed 11 / 11 datasets\n",
            "   Ready for preprocess_data() function\n",
            "\n",
            "ðŸ“‹ Summary:\n",
            "============================================================\n",
            "Healthy                  :  12236 samples Ã—  2179 features\n",
            "COVID19                  :   1813 samples Ã—  1273 features\n",
            "Colorectal_Neoplasms     :    793 samples Ã—  1377 features\n",
            "Crohn_s                  :   1425 samples Ã—  1184 features\n",
            "Diabetes                 :    326 samples Ã—  1053 features\n",
            "IBS                      :    379 samples Ã—   500 features\n",
            "KidneyFailure            :    347 samples Ã—   896 features\n",
            "Parkinsons               :    549 samples Ã—  1053 features\n",
            "Ulcerative_Colitis       :    859 samples Ã—  1055 features\n",
            "cysticfibrosis           :    488 samples Ã—   953 features\n",
            "nafld                    :    493 samples Ã—   944 features\n",
            "============================================================\n",
            "Total: 19708 samples, up to 2179 unique taxa\n",
            "\n",
            "âœ… Data is now in wide format and ready for preprocessing!\n"
          ]
        }
      ],
      "source": [
        "## ðŸ”„ Cell 3.5: Pivot Long Format to Wide Format (INSERT THIS AFTER CELL 3)\n",
        "\n",
        "def pivot_long_to_wide(all_data, verbose=True):\n",
        "    \"\"\"\n",
        "    Convert microbiome data from long format to wide format.\n",
        "\n",
        "    Long format: Each row = one taxon abundance in one sample\n",
        "    Wide format: Each row = one sample with all taxon abundances as columns\n",
        "\n",
        "    Args:\n",
        "        all_data: List of (dataframe, label) tuples in long format\n",
        "        verbose: Print progress information\n",
        "\n",
        "    Returns:\n",
        "        processed_data: List of (dataframe, label) tuples in wide format\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\nðŸ”„ Pivoting data from long to wide format...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    processed_data = []\n",
        "\n",
        "    for i, (df, label) in enumerate(all_data):\n",
        "        # Make a copy to avoid modifying original\n",
        "        df_copy = df.copy()\n",
        "\n",
        "        # Normalize column names (lowercase, replace spaces with underscores)\n",
        "        df_copy.columns = df_copy.columns.str.lower().str.replace(' ', '_')\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nðŸ“Š Processing {label}:\")\n",
        "            print(f\"   Original shape: {df.shape}\")\n",
        "            print(f\"   Columns: {list(df_copy.columns)}\")\n",
        "\n",
        "        # Verify required columns exist\n",
        "        required_cols = ['run_id', 'ncbi_taxon_id', 'relative_abundance']\n",
        "        missing_cols = [col for col in required_cols if col not in df_copy.columns]\n",
        "\n",
        "        if missing_cols:\n",
        "            print(f\"   âš ï¸  WARNING: Missing columns {missing_cols}. Skipping {label}.\")\n",
        "            continue\n",
        "\n",
        "        # Get unique counts before pivot\n",
        "        n_samples = df_copy['run_id'].nunique()\n",
        "        n_taxa = df_copy['ncbi_taxon_id'].nunique()\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   Unique samples (run_id): {n_samples}\")\n",
        "            print(f\"   Unique taxa (ncbi_taxon_id): {n_taxa}\")\n",
        "\n",
        "        try:\n",
        "            # Pivot the data\n",
        "            df_wide = df_copy.pivot_table(\n",
        "                index='run_id',\n",
        "                columns='ncbi_taxon_id',\n",
        "                values='relative_abundance',\n",
        "                fill_value=0.0,\n",
        "                aggfunc='first'  # In case of duplicates, take first value\n",
        "            )\n",
        "\n",
        "            # Rename columns to be more descriptive\n",
        "            df_wide.columns = [f'taxon_{col}' for col in df_wide.columns]\n",
        "\n",
        "            # Reset index to make run_id a column\n",
        "            df_wide = df_wide.reset_index()\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"   âœ“ Pivoted shape: {df_wide.shape}\")\n",
        "                print(f\"   âœ“ Samples: {len(df_wide)}, Features: {len(df_wide.columns) - 1}\")\n",
        "\n",
        "            # Add to processed data\n",
        "            processed_data.append((df_wide, label))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Error pivoting {label}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(f\"âœ… Pivoting complete!\")\n",
        "        print(f\"   Processed {len(processed_data)} / {len(all_data)} datasets\")\n",
        "        print(f\"   Ready for preprocess_data() function\")\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "\n",
        "# Run the pivot operation\n",
        "processed_data = pivot_long_to_wide(all_data, verbose=True)\n",
        "\n",
        "# Replace all_data with processed_data\n",
        "all_data = processed_data\n",
        "\n",
        "# Summary\n",
        "print(\"\\nðŸ“‹ Summary:\")\n",
        "print(\"=\" * 60)\n",
        "total_samples = 0\n",
        "total_features = 0\n",
        "\n",
        "for df, label in all_data:\n",
        "    n_samples = len(df)\n",
        "    n_features = len(df.columns) - 1  # Exclude run_id\n",
        "    total_samples += n_samples\n",
        "    total_features = max(total_features, n_features)\n",
        "    print(f\"{label:25s}: {n_samples:6d} samples Ã— {n_features:5d} features\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total: {total_samples} samples, up to {total_features} unique taxa\")\n",
        "print(f\"\\nâœ… Data is now in wide format and ready for preprocessing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANYxcNhjWGX-"
      },
      "source": [
        "## ðŸ§¹ Cell 4: Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RolCP0whWGX_",
        "outputId": "74ec69ba-7825-45a0-b973-05837f7e057c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”§ Starting preprocessing pipeline...\n",
            "============================================================\n",
            "âœ“ Total unique features found: 2565\n",
            "âœ“ Dropping identifier columns: {'run_id', 'Sample_ID', 'SampleID', 'Run_ID', 'ID'}\n",
            "âœ“ Features after filtering: 2564\n",
            "\n",
            "âœ“ Combined dataset shape: (19708, 2564)\n",
            "âœ“ This represents 19708 samples (run_ids)\n",
            "âœ“ Remaining numeric features: 2564\n",
            "âœ“ Removed 186 low-variance features\n",
            "âœ“ Remaining features: 2378\n",
            "âœ“ Applied log transformation: log(x + 1e-06)\n",
            "\n",
            "âœ“ Label encoding:\n",
            "   0: COVID19 (1813 samples)\n",
            "   1: Colorectal_Neoplasms (793 samples)\n",
            "   2: Crohn_s (1425 samples)\n",
            "   3: Diabetes (326 samples)\n",
            "   4: Healthy (12236 samples)\n",
            "   5: IBS (379 samples)\n",
            "   6: KidneyFailure (347 samples)\n",
            "   7: Parkinsons (549 samples)\n",
            "   8: Ulcerative_Colitis (859 samples)\n",
            "   9: cysticfibrosis (488 samples)\n",
            "   10: nafld (493 samples)\n",
            "============================================================\n",
            "\n",
            "âœ… Preprocessing complete!\n",
            "   Final shape: (19708, 2378)\n",
            "   Samples (actual run_ids): 19708\n",
            "   Features (microbial taxa): 2378\n",
            "   Classes: 11\n",
            "\n",
            "ðŸ“Š Class Distribution:\n",
            "============================================================\n",
            "COVID19                  :   1813 samples (  9.20%)\n",
            "Colorectal_Neoplasms     :    793 samples (  4.02%)\n",
            "Crohn_s                  :   1425 samples (  7.23%)\n",
            "Diabetes                 :    326 samples (  1.65%)\n",
            "Healthy                  :  12236 samples ( 62.09%)\n",
            "IBS                      :    379 samples (  1.92%)\n",
            "KidneyFailure            :    347 samples (  1.76%)\n",
            "Parkinsons               :    549 samples (  2.79%)\n",
            "Ulcerative_Colitis       :    859 samples (  4.36%)\n",
            "cysticfibrosis           :    488 samples (  2.48%)\n",
            "nafld                    :    493 samples (  2.50%)\n"
          ]
        }
      ],
      "source": [
        "## ðŸ”§ Cell 4: Updated Preprocessing (REPLACE CELL 4 WITH THIS)\n",
        "\n",
        "def preprocess_data(all_data, disease_names, config, verbose=True):\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline for WIDE FORMAT data:\n",
        "    1. Align features across all datasets\n",
        "    2. Drop identifier columns (run_id, etc.)\n",
        "    3. Remove zero-variance features\n",
        "    4. Log-transform abundances\n",
        "    5. Combine into single DataFrame\n",
        "\n",
        "    Returns:\n",
        "        X: Feature matrix (numpy array)\n",
        "        y: Labels (numpy array)\n",
        "        feature_names: List of feature column names\n",
        "        label_encoder: Fitted LabelEncoder\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\nðŸ”§ Starting preprocessing pipeline...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: Collect all unique features\n",
        "    all_features = set()\n",
        "    for df, _ in all_data:\n",
        "        all_features.update(df.columns)\n",
        "\n",
        "    # Remove identifier columns (including run_id from pivot)\n",
        "    drop_cols = set(config['drop_columns'])\n",
        "    drop_cols.add('run_id')  # Add run_id from pivot operation\n",
        "\n",
        "    features_to_keep = sorted(all_features - drop_cols)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"âœ“ Total unique features found: {len(all_features)}\")\n",
        "        print(f\"âœ“ Dropping identifier columns: {drop_cols}\")\n",
        "        print(f\"âœ“ Features after filtering: {len(features_to_keep)}\")\n",
        "\n",
        "    # Step 2: Align all datasets to common feature set\n",
        "    aligned_data = []\n",
        "    labels = []\n",
        "\n",
        "    for df, label in all_data:\n",
        "        # Create aligned dataframe with float type\n",
        "        df_aligned = pd.DataFrame(0.0, index=df.index, columns=features_to_keep)\n",
        "\n",
        "        # Fill in existing features\n",
        "        for col in features_to_keep:\n",
        "            if col in df.columns:\n",
        "                df_aligned[col] = df[col].values\n",
        "\n",
        "        aligned_data.append(df_aligned)\n",
        "        labels.extend([label] * len(df))\n",
        "\n",
        "    # Step 3: Combine all data\n",
        "    X_df = pd.concat(aligned_data, axis=0, ignore_index=True)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nâœ“ Combined dataset shape: {X_df.shape}\")\n",
        "        print(f\"âœ“ This represents {len(X_df)} samples (run_ids)\")\n",
        "\n",
        "    # Step 3.5: Drop any remaining non-numeric columns (safety check)\n",
        "    numeric_cols = []\n",
        "    non_numeric_cols = []\n",
        "\n",
        "    for col in X_df.columns:\n",
        "        try:\n",
        "            pd.to_numeric(X_df[col], errors='raise')\n",
        "            numeric_cols.append(col)\n",
        "        except (ValueError, TypeError):\n",
        "            non_numeric_cols.append(col)\n",
        "\n",
        "    if non_numeric_cols:\n",
        "        if verbose:\n",
        "            print(f\"\\nâš ï¸  Found {len(non_numeric_cols)} non-numeric columns:\")\n",
        "            for col in non_numeric_cols[:5]:\n",
        "                sample_value = X_df[col].iloc[0] if len(X_df) > 0 else \"N/A\"\n",
        "                print(f\"   - {col}: (example: {sample_value})\")\n",
        "            if len(non_numeric_cols) > 5:\n",
        "                print(f\"   ... and {len(non_numeric_cols) - 5} more\")\n",
        "\n",
        "        X_df = X_df[numeric_cols]\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"âœ“ Dropped {len(non_numeric_cols)} non-numeric columns\")\n",
        "\n",
        "    # Ensure all numeric\n",
        "    X_df = X_df.astype(float)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"âœ“ Remaining numeric features: {X_df.shape[1]}\")\n",
        "\n",
        "    # Step 4: Remove zero/low variance features\n",
        "    variances = X_df.var()\n",
        "    low_var_cols = variances[variances < config['min_variance_threshold']].index\n",
        "    X_df = X_df.drop(columns=low_var_cols)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"âœ“ Removed {len(low_var_cols)} low-variance features\")\n",
        "        print(f\"âœ“ Remaining features: {X_df.shape[1]}\")\n",
        "\n",
        "    # Step 5: Log transformation\n",
        "    if config['log_transform']:\n",
        "        X_df = np.log(X_df + config['log_constant'])\n",
        "        if verbose:\n",
        "            print(f\"âœ“ Applied log transformation: log(x + {config['log_constant']})\")\n",
        "\n",
        "    # Step 6: Convert to numpy arrays\n",
        "    X = X_df.values\n",
        "    feature_names = X_df.columns.tolist()\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y = label_encoder.fit_transform(labels)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nâœ“ Label encoding:\")\n",
        "        for idx, name in enumerate(label_encoder.classes_):\n",
        "            print(f\"   {idx}: {name} ({np.sum(y == idx)} samples)\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"\\nâœ… Preprocessing complete!\")\n",
        "        print(f\"   Final shape: {X.shape}\")\n",
        "        print(f\"   Samples (actual run_ids): {len(y)}\")\n",
        "        print(f\"   Features (microbial taxa): {len(feature_names)}\")\n",
        "        print(f\"   Classes: {len(np.unique(y))}\")\n",
        "\n",
        "    return X, y, feature_names, label_encoder\n",
        "\n",
        "\n",
        "# Run preprocessing\n",
        "X_raw, y, feature_names, label_encoder = preprocess_data(\n",
        "    all_data,\n",
        "    disease_names,\n",
        "    CONFIG\n",
        ")\n",
        "\n",
        "# Print class distribution\n",
        "print(\"\\nðŸ“Š Class Distribution:\")\n",
        "print(\"=\" * 60)\n",
        "class_counts = Counter(y)\n",
        "for class_idx in sorted(class_counts.keys()):\n",
        "    class_name = label_encoder.inverse_transform([class_idx])[0]\n",
        "    count = class_counts[class_idx]\n",
        "    percentage = 100 * count / len(y)\n",
        "    print(f\"{class_name:25s}: {count:6d} samples ({percentage:6.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2lxLtNwWGX_"
      },
      "source": [
        "## ðŸŽ¯ Cell 5: Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT-qcHsFWGX_",
        "outputId": "21e7a3bf-688c-4f12-9c4e-392a3e44f655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¬ Feature Selection Pipeline\n",
            "============================================================\n",
            "Method: f_classif\n",
            "Selecting top 500 features from 2378\n",
            "\n",
            "âœ“ Selected 500 features\n",
            "âœ“ New feature matrix shape: (19708, 500)\n",
            "\n",
            "ðŸ“ˆ Top 10 features by score:\n",
            "    1. taxon_28131                                        (score: 421.69)\n",
            "    2. taxon_853                                          (score: 312.52)\n",
            "    3. taxon_341694                                       (score: 299.86)\n",
            "    4. taxon_745368                                       (score: 297.64)\n",
            "    5. taxon_1352                                         (score: 295.28)\n",
            "    6. taxon_2981769                                      (score: 272.89)\n",
            "    7. taxon_39950                                        (score: 268.11)\n",
            "    8. taxon_351091                                       (score: 262.49)\n",
            "    9. taxon_1628085                                      (score: 261.45)\n",
            "   10. taxon_2885350                                      (score: 244.85)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "def select_top_features(X, y, feature_names, n_features, method='f_classif', verbose=True):\n",
        "    \"\"\"\n",
        "    Perform feature selection using ANOVA F-test or Mutual Information.\n",
        "\n",
        "    Args:\n",
        "        X: Feature matrix\n",
        "        y: Labels\n",
        "        feature_names: List of feature names\n",
        "        n_features: Number of top features to select\n",
        "        method: 'f_classif' or 'mutual_info'\n",
        "\n",
        "    Returns:\n",
        "        X_selected: Selected features\n",
        "        selector: Fitted SelectKBest object\n",
        "        selected_feature_names: Names of selected features\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\nðŸ”¬ Feature Selection Pipeline\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Method: {method}\")\n",
        "        print(f\"Selecting top {n_features} features from {X.shape[1]}\")\n",
        "\n",
        "    # Choose scoring function\n",
        "    if method == 'f_classif':\n",
        "        score_func = f_classif\n",
        "    elif method == 'mutual_info':\n",
        "        score_func = mutual_info_classif\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown method: {method}\")\n",
        "\n",
        "    # Perform feature selection\n",
        "    n_features_to_select = min(n_features, X.shape[1])\n",
        "    selector = SelectKBest(score_func=score_func, k=n_features_to_select)\n",
        "    X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "    # Get selected feature names\n",
        "    selected_indices = selector.get_support(indices=True)\n",
        "    selected_feature_names = [feature_names[i] for i in selected_indices]\n",
        "\n",
        "    # Get feature scores\n",
        "    scores = selector.scores_[selected_indices]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nâœ“ Selected {len(selected_feature_names)} features\")\n",
        "        print(f\"âœ“ New feature matrix shape: {X_selected.shape}\")\n",
        "        print(f\"\\nðŸ“ˆ Top 10 features by score:\")\n",
        "        top_10_idx = np.argsort(scores)[-10:][::-1]\n",
        "        for i, idx in enumerate(top_10_idx, 1):\n",
        "            print(f\"   {i:2d}. {selected_feature_names[idx]:50s} (score: {scores[idx]:.2f})\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    return X_selected, selector, selected_feature_names\n",
        "\n",
        "\n",
        "# Perform feature selection\n",
        "X_selected, feature_selector, selected_features = select_top_features(\n",
        "    X_raw,\n",
        "    y,\n",
        "    feature_names,\n",
        "    CONFIG['n_features'],\n",
        "    CONFIG['feature_selection_method']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07klKkg8WGX_"
      },
      "source": [
        "## âš–ï¸ Cell 6: Train/Val/Test Split with Stratification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ijVeN8vxNtJa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4ebba0-9739-47e8-e1f5-42cb30ca816c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âš–ï¸ Undersampling Healthy Class to Reduce Imbalance\n",
            "============================================================\n",
            "Before - Healthy: 12,236, Diseased: 7,472\n",
            "         Ratio: 1.64:1\n",
            "After  - Healthy: 7,472, Diseased: 7,472\n",
            "         New Ratio: 1.00:1\n",
            "âœ… Removed 4,764 healthy samples\n",
            "============================================================\n",
            "\n",
            "âœ… Dataset after undersampling: (14944, 500)\n",
            "âœ… Kept 14944 samples\n",
            "âœ… X_raw also undersampled: (14944, 2378)\n",
            "\n",
            "ðŸ”„ Creating train/val/test splits from undersampled data...\n",
            "\n",
            "ðŸ“Š Creating Train/Val/Test Split\n",
            "============================================================\n",
            "Train set: 10460 samples (70.0%)\n",
            "Val set:   2242 samples (15.0%)\n",
            "Test set:  2242 samples (15.0%)\n",
            "\n",
            "ðŸ“Š Class distribution per split:\n",
            "\n",
            "Train:\n",
            "   Class 0: 1269 samples ( 12.1%)\n",
            "   Class 1:  555 samples (  5.3%)\n",
            "   Class 2:  997 samples (  9.5%)\n",
            "   Class 3:  228 samples (  2.2%)\n",
            "   Class 4: 5230 samples ( 50.0%)\n",
            "   Class 5:  265 samples (  2.5%)\n",
            "   Class 6:  243 samples (  2.3%)\n",
            "   Class 7:  385 samples (  3.7%)\n",
            "   Class 8:  601 samples (  5.7%)\n",
            "   Class 9:  342 samples (  3.3%)\n",
            "   Class 10:  345 samples (  3.3%)\n",
            "\n",
            "Val:\n",
            "   Class 0:  272 samples ( 12.1%)\n",
            "   Class 1:  119 samples (  5.3%)\n",
            "   Class 2:  214 samples (  9.5%)\n",
            "   Class 3:   49 samples (  2.2%)\n",
            "   Class 4: 1121 samples ( 50.0%)\n",
            "   Class 5:   57 samples (  2.5%)\n",
            "   Class 6:   52 samples (  2.3%)\n",
            "   Class 7:   82 samples (  3.7%)\n",
            "   Class 8:  129 samples (  5.8%)\n",
            "   Class 9:   73 samples (  3.3%)\n",
            "   Class 10:   74 samples (  3.3%)\n",
            "\n",
            "Test:\n",
            "   Class 0:  272 samples ( 12.1%)\n",
            "   Class 1:  119 samples (  5.3%)\n",
            "   Class 2:  214 samples (  9.5%)\n",
            "   Class 3:   49 samples (  2.2%)\n",
            "   Class 4: 1121 samples ( 50.0%)\n",
            "   Class 5:   57 samples (  2.5%)\n",
            "   Class 6:   52 samples (  2.3%)\n",
            "   Class 7:   82 samples (  3.7%)\n",
            "   Class 8:  129 samples (  5.8%)\n",
            "   Class 9:   73 samples (  3.3%)\n",
            "   Class 10:   74 samples (  3.3%)\n",
            "============================================================\n",
            "âœ… Splits created with balanced data\n"
          ]
        }
      ],
      "source": [
        "# Cell 6.5: Undersample THEN Split (Combined)\n",
        "\n",
        "def undersample_majority_class(X, y, healthy_idx=4, ratio=2.0, random_state=42):\n",
        "    \"\"\"\n",
        "    Undersample the Healthy class to reduce severe imbalance.\n",
        "    Returns both undersampled data AND the indices that were kept.\n",
        "    \"\"\"\n",
        "    print(\"\\nâš–ï¸ Undersampling Healthy Class to Reduce Imbalance\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Split into healthy and diseased\n",
        "    healthy_mask = (y == healthy_idx)\n",
        "    diseased_mask = ~healthy_mask\n",
        "\n",
        "    healthy_indices = np.where(healthy_mask)[0]\n",
        "    diseased_indices = np.where(diseased_mask)[0]\n",
        "\n",
        "    print(f\"Before - Healthy: {len(healthy_indices):,}, Diseased: {len(diseased_indices):,}\")\n",
        "    print(f\"         Ratio: {len(healthy_indices)/len(diseased_indices):.2f}:1\")\n",
        "\n",
        "    # Calculate target healthy samples\n",
        "    n_diseased = len(diseased_indices)\n",
        "    n_healthy_target = int(n_diseased * ratio)\n",
        "\n",
        "    # Randomly sample healthy INDICES\n",
        "    np.random.seed(random_state)\n",
        "    if n_healthy_target < len(healthy_indices):\n",
        "        sampled_healthy_indices = np.random.choice(healthy_indices, n_healthy_target, replace=False)\n",
        "    else:\n",
        "        sampled_healthy_indices = healthy_indices\n",
        "\n",
        "    # Combine all kept indices\n",
        "    kept_indices = np.concatenate([sampled_healthy_indices, diseased_indices])\n",
        "\n",
        "    # Shuffle\n",
        "    np.random.shuffle(kept_indices)\n",
        "\n",
        "    # Extract data\n",
        "    X_balanced = X[kept_indices]\n",
        "    y_balanced = y[kept_indices]\n",
        "\n",
        "    print(f\"After  - Healthy: {n_healthy_target:,}, Diseased: {n_diseased:,}\")\n",
        "    print(f\"         New Ratio: {n_healthy_target/n_diseased:.2f}:1\")\n",
        "    print(f\"âœ… Removed {len(healthy_indices) - n_healthy_target:,} healthy samples\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return X_balanced, y_balanced, kept_indices\n",
        "\n",
        "\n",
        "def create_train_val_test_split(X, y, test_size=0.15, val_size=0.15, random_state=42, verbose=True):\n",
        "    \"\"\"\n",
        "    Create stratified train/validation/test splits.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing train/val/test splits\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\nðŸ“Š Creating Train/Val/Test Split\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    # First split: train+val vs test\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=test_size,\n",
        "        stratify=y,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    # Second split: train vs val\n",
        "    val_size_adjusted = val_size / (1 - test_size)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp,\n",
        "        test_size=val_size_adjusted,\n",
        "        stratify=y_temp,\n",
        "        random_state=random_state\n",
        "    )\n",
        "\n",
        "    splits = {\n",
        "        'X_train': X_train,\n",
        "        'y_train': y_train,\n",
        "        'X_val': X_val,\n",
        "        'y_val': y_val,\n",
        "        'X_test': X_test,\n",
        "        'y_test': y_test\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Train set: {X_train.shape[0]} samples ({100*len(X_train)/len(X):.1f}%)\")\n",
        "        print(f\"Val set:   {X_val.shape[0]} samples ({100*len(X_val)/len(X):.1f}%)\")\n",
        "        print(f\"Test set:  {X_test.shape[0]} samples ({100*len(X_test)/len(X):.1f}%)\")\n",
        "\n",
        "        print(\"\\nðŸ“Š Class distribution per split:\")\n",
        "        for split_name, split_y in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:\n",
        "            print(f\"\\n{split_name}:\")\n",
        "            unique, counts = np.unique(split_y, return_counts=True)\n",
        "            for u, c in zip(unique, counts):\n",
        "                print(f\"   Class {u}: {c:4d} samples ({100*c/len(split_y):5.1f}%)\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    return splits\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# STEP 1: UNDERSAMPLE (if enabled)\n",
        "# ============================================================\n",
        "if CONFIG['undersample_healthy']:\n",
        "    X_selected, y, kept_indices = undersample_majority_class(\n",
        "        X_selected,\n",
        "        y,\n",
        "        healthy_idx=4,\n",
        "        ratio=CONFIG['undersample_ratio'],\n",
        "        random_state=CONFIG['random_state']\n",
        "    )\n",
        "\n",
        "    print(f\"\\nâœ… Dataset after undersampling: {X_selected.shape}\")\n",
        "    print(f\"âœ… Kept {len(kept_indices)} samples\")\n",
        "\n",
        "    # CRITICAL: Also undersample X_raw using the same indices\n",
        "    X_raw = X_raw[kept_indices]\n",
        "    print(f\"âœ… X_raw also undersampled: {X_raw.shape}\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Undersampling disabled in CONFIG\")\n",
        "\n",
        "# ============================================================\n",
        "# STEP 2: SPLIT DATA (after undersampling!)\n",
        "# ============================================================\n",
        "print(\"\\nðŸ”„ Creating train/val/test splits from undersampled data...\")\n",
        "data_splits = create_train_val_test_split(\n",
        "    X_selected,\n",
        "    y,\n",
        "    test_size=CONFIG['test_size'],\n",
        "    val_size=CONFIG['val_size'],\n",
        "    random_state=CONFIG['random_state'],\n",
        "    verbose=True\n",
        ")\n",
        "print(\"âœ… Splits created with balanced data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUqOU7CCWGX_"
      },
      "source": [
        "## ðŸ”„ Cell 7: Normalization (StandardScaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eolgocnBWGYA",
        "outputId": "a936e6d6-e7c3-41cd-862b-09d918322b56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Applied StandardScaler normalization\n",
            "   Training mean: 0.000000\n",
            "   Training std:  1.000000\n"
          ]
        }
      ],
      "source": [
        "# Fit scaler on training data only\n",
        "scaler = StandardScaler()\n",
        "data_splits['X_train'] = scaler.fit_transform(data_splits['X_train'])\n",
        "data_splits['X_val'] = scaler.transform(data_splits['X_val'])\n",
        "data_splits['X_test'] = scaler.transform(data_splits['X_test'])\n",
        "\n",
        "print(\"\\nâœ… Applied StandardScaler normalization\")\n",
        "print(f\"   Training mean: {data_splits['X_train'].mean():.6f}\")\n",
        "print(f\"   Training std:  {data_splits['X_train'].std():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8.5: Focal Loss Implementation\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Focal Loss for addressing class imbalance.\n",
        "    Focuses training on hard examples.\n",
        "\n",
        "    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "print(\"âœ… Focal Loss class defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ftWgq8Wm_89",
        "outputId": "c8514cdc-8974-464d-93e3-cfc95d6ddd36"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Focal Loss class defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T52CbZejWGYA"
      },
      "source": [
        "## âš–ï¸ Cell 8: Handle Class Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqqes4tzWGYA",
        "outputId": "12a21980-09d8-4fe5-db6a-1cc1ded0daf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âš–ï¸  Class Weights (balanced):\n",
            "============================================================\n",
            "   COVID19                  : 0.7493\n",
            "   Colorectal_Neoplasms     : 1.7133\n",
            "   Crohn_s                  : 0.9538\n",
            "   Diabetes                 : 4.1707\n",
            "   Healthy                  : 0.1818\n",
            "   IBS                      : 3.5883\n",
            "   KidneyFailure            : 3.9132\n",
            "   Parkinsons               : 2.4699\n",
            "   Ulcerative_Colitis       : 1.5822\n",
            "   cysticfibrosis           : 2.7804\n",
            "   nafld                    : 2.7563\n",
            "============================================================\n",
            "\n",
            "âš–ï¸  Binary Class Weights (Healthy vs Diseased):\n",
            "   Healthy:  1.0000\n",
            "   Diseased: 1.0000\n"
          ]
        }
      ],
      "source": [
        "def compute_class_weights(y, verbose=True):\n",
        "    \"\"\"\n",
        "    Compute class weights inversely proportional to class frequencies.\n",
        "    \"\"\"\n",
        "    weights = class_weight.compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y),\n",
        "        y=y\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nâš–ï¸  Class Weights (balanced):\")\n",
        "        print(\"=\" * 60)\n",
        "        for class_idx, weight in enumerate(weights):\n",
        "            class_name = label_encoder.inverse_transform([class_idx])[0]\n",
        "            print(f\"   {class_name:25s}: {weight:.4f}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "    return torch.FloatTensor(weights)\n",
        "\n",
        "\n",
        "def create_weighted_sampler(y, verbose=True):\n",
        "    \"\"\"\n",
        "    Create WeightedRandomSampler for oversampling rare classes.\n",
        "    \"\"\"\n",
        "    class_counts = np.bincount(y)\n",
        "    weights = 1.0 / class_counts[y]\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights=weights,\n",
        "        num_samples=len(weights),\n",
        "        replacement=True\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\nâœ… Created WeightedRandomSampler for balanced sampling\")\n",
        "\n",
        "    return sampler\n",
        "\n",
        "\n",
        "# Compute class weights for all classes (multi-class)\n",
        "class_weights_multiclass = compute_class_weights(data_splits['y_train'])\n",
        "\n",
        "# Compute class weights for binary (Healthy vs Diseased)\n",
        "y_train_binary = (data_splits['y_train'] !=4).astype(int)  # 0=Healthy, 1=Diseased\n",
        "class_weights_binary = compute_class_weights(y_train_binary, verbose=False)\n",
        "\n",
        "print(\"\\nâš–ï¸  Binary Class Weights (Healthy vs Diseased):\")\n",
        "print(f\"   Healthy:  {class_weights_binary[0]:.4f}\")\n",
        "print(f\"   Diseased: {class_weights_binary[1]:.4f}\")\n",
        "\n",
        "# Optionally create weighted sampler\n",
        "if CONFIG['use_weighted_sampler']:\n",
        "    train_sampler = create_weighted_sampler(data_splits['y_train'])\n",
        "else:\n",
        "    train_sampler = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzBvauQGWGYA"
      },
      "source": [
        "## ðŸ“¦ Cell 9: PyTorch Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-h-F01vWGYA",
        "outputId": "fa012639-c0d3-44d5-9183-6ce13945e1d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Dataset and DataLoader classes defined\n"
          ]
        }
      ],
      "source": [
        "class MicrobiomeDataset(Dataset):\n",
        "    \"\"\"Custom PyTorch Dataset for microbiome data.\"\"\"\n",
        "\n",
        "    def __init__(self, X, y, binary_labels=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            X: Feature matrix (numpy array)\n",
        "            y: Labels (numpy array)\n",
        "            binary_labels: If True, convert to binary (0=Healthy, 1=Diseased)\n",
        "        \"\"\"\n",
        "        self.X = torch.FloatTensor(X)\n",
        "\n",
        "        if binary_labels:\n",
        "\n",
        "            healthy_idx = 4\n",
        "            y_binary = (y != healthy_idx).astype(int)\n",
        "            self.y = torch.LongTensor(y_binary)\n",
        "        else:\n",
        "            self.y = torch.LongTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "def create_dataloaders(splits, batch_size, sampler=None, binary=False):\n",
        "    \"\"\"\n",
        "    Create PyTorch DataLoaders for train/val/test sets.\n",
        "    \"\"\"\n",
        "    train_dataset = MicrobiomeDataset(\n",
        "        splits['X_train'],\n",
        "        splits['y_train'],\n",
        "        binary_labels=binary\n",
        "    )\n",
        "    val_dataset = MicrobiomeDataset(\n",
        "        splits['X_val'],\n",
        "        splits['y_val'],\n",
        "        binary_labels=binary\n",
        "    )\n",
        "    test_dataset = MicrobiomeDataset(\n",
        "        splits['X_test'],\n",
        "        splits['y_test'],\n",
        "        binary_labels=binary\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=(sampler is None),\n",
        "        sampler=sampler\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "\n",
        "print(\"\\nâœ… Dataset and DataLoader classes defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmsyqMg1WGYA"
      },
      "source": [
        "## ðŸ§  Cell 10: Model Architecture (Shared MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOha2wpWWGYA",
        "outputId": "16e9a770-ee4d-4367-c2ad-c9ca73ed74a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ§  Model Architecture:\n",
            "============================================================\n",
            "Input dimension: 500\n",
            "Hidden layers: [1024, 512, 256]\n",
            "Dropout: 0.3\n",
            "Batch normalization: True\n",
            "\n",
            "Binary output: 2 classes (Healthy vs Diseased)\n",
            "Multi-class output: 11 classes\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "class DiseaseClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-layer Perceptron (MLP) for disease classification.\n",
        "    Can be used for both binary and multi-class classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, num_classes, hidden_dims=[512, 256, 128],\n",
        "                 dropout=0.3, use_batch_norm=True):\n",
        "        super(DiseaseClassifier, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_dim = input_dim\n",
        "\n",
        "        # Hidden layers\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
        "\n",
        "            if use_batch_norm:\n",
        "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(prev_dim, num_classes))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "# Print model architecture\n",
        "input_dim = data_splits['X_train'].shape[1]\n",
        "num_classes = len(np.unique(y))\n",
        "\n",
        "print(\"\\nðŸ§  Model Architecture:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Input dimension: {input_dim}\")\n",
        "print(f\"Hidden layers: {CONFIG['hidden_dims']}\")\n",
        "print(f\"Dropout: {CONFIG['dropout']}\")\n",
        "print(f\"Batch normalization: {CONFIG['use_batch_norm']}\")\n",
        "print(f\"\\nBinary output: 2 classes (Healthy vs Diseased)\")\n",
        "print(f\"Multi-class output: {num_classes} classes\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyyZ4UM_WGYB"
      },
      "source": [
        "## ðŸ‹ï¸ Cell 11: Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDGs5GvFWGYB",
        "outputId": "6bef8740-f369-4037-afc1-ed1324bc914b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Updated training function defined\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer,\n",
        "                num_epochs, patience, device, scheduler=None,\n",
        "                warmup_epochs=0, model_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Train a PyTorch model with early stopping and optional LR scheduling.\n",
        "\n",
        "    New features:\n",
        "    - Learning rate warmup\n",
        "    - LR scheduler support\n",
        "    - Better progress reporting\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸ‹ï¸  Training {model_name}...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    model = model.to(device)\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_f1 = 0.0  # NEW: Also track best F1\n",
        "    best_model_state = None\n",
        "    patience_counter = 0\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': [],\n",
        "        'learning_rates': []  # NEW: Track LR\n",
        "    }\n",
        "\n",
        "    # Get base learning rate for warmup\n",
        "    base_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # ========== NEW: Learning rate warmup ==========\n",
        "        if epoch < warmup_epochs:\n",
        "            warmup_lr = base_lr * (epoch + 1) / warmup_epochs\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = warmup_lr\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # NEW: Gradient clipping (prevents exploding gradients)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            train_loss += loss.item() * batch_X.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_correct += (predicted == batch_y).sum().item()\n",
        "            train_total += batch_y.size(0)\n",
        "\n",
        "        train_loss = train_loss / train_total\n",
        "        train_acc = train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "\n",
        "                val_loss += loss.item() * batch_X.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_correct += (predicted == batch_y).sum().item()\n",
        "                val_total += batch_y.size(0)\n",
        "\n",
        "                val_preds.extend(predicted.cpu().numpy())\n",
        "                val_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "        val_loss = val_loss / val_total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        # NEW: Compute validation F1\n",
        "        val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "\n",
        "        # Get current learning rate\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Store history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        history['learning_rates'].append(current_lr)\n",
        "\n",
        "        # Print progress\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "            print(f\"Epoch [{epoch+1:3d}/{num_epochs}] \"\n",
        "                  f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
        "                  f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} F1: {val_f1:.4f} | \"\n",
        "                  f\"LR: {current_lr:.6f}\")\n",
        "\n",
        "        # ========== NEW: LR Scheduler step ==========\n",
        "        if scheduler is not None:\n",
        "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                scheduler.step(val_loss)\n",
        "            else:\n",
        "                scheduler.step()\n",
        "\n",
        "        # ========== UPDATED: Early stopping on F1 score ==========\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_loss = val_loss\n",
        "            best_val_f1 = val_f1\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"\\nâ¹ï¸  Early stopping at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    # Load best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"âœ… Training complete!\")\n",
        "    print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
        "    print(f\"   Best validation F1: {best_val_f1:.4f}\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "print(\"\\nâœ… Updated training function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUQNF5GdWGYB"
      },
      "source": [
        "## ðŸ“Š Cell 12: Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SyQlqTSWGYB",
        "outputId": "bab75966-0d4b-43e4-851d-dad8e0210266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Evaluation function defined\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, test_loader, device, class_names, stage_name=\"Model\"):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of a trained model.\n",
        "\n",
        "    Returns:\n",
        "        results: Dictionary containing all metrics\n",
        "    \"\"\"\n",
        "    print(f\"\\nðŸ“Š Evaluating {stage_name}...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            batch_X = batch_X.to(device)\n",
        "            outputs = model(batch_X)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(batch_y.numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    # Per-class metrics\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average=None, zero_division=0\n",
        "    )\n",
        "\n",
        "    # Macro/Micro averages\n",
        "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    micro_f1 = f1_score(all_labels, all_preds, average='micro')\n",
        "\n",
        "    # AUROC\n",
        "    if len(class_names) > 2:\n",
        "        # Multi-class AUROC\n",
        "        auroc_macro = roc_auc_score(\n",
        "            all_labels, all_probs,\n",
        "            average='macro',\n",
        "            multi_class='ovr'\n",
        "        )\n",
        "        auroc_micro = roc_auc_score(\n",
        "            all_labels, all_probs,\n",
        "            average='micro',\n",
        "            multi_class='ovr'\n",
        "        )\n",
        "    else:\n",
        "        # Binary AUROC\n",
        "        auroc_macro = roc_auc_score(all_labels, all_probs[:, 1])\n",
        "        auroc_micro = auroc_macro\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nðŸŽ¯ Overall Metrics:\")\n",
        "    print(f\"   Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"   Macro F1:  {macro_f1:.4f}\")\n",
        "    print(f\"   Micro F1:  {micro_f1:.4f}\")\n",
        "    print(f\"   AUROC (Macro): {auroc_macro:.4f}\")\n",
        "    print(f\"   AUROC (Micro): {auroc_micro:.4f}\")\n",
        "\n",
        "    print(f\"\\nðŸ“Š Per-Class Metrics:\")\n",
        "    print(f\"{'Class':<20s} {'Precision':>10s} {'Recall':>10s} {'F1':>10s} {'Support':>10s}\")\n",
        "    print(\"-\" * 60)\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"{class_name:<20s} {precision[i]:>10.4f} {recall[i]:>10.4f} \"\n",
        "              f\"{f1[i]:>10.4f} {support[i]:>10d}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    results = {\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels,\n",
        "        'probabilities': all_probs,\n",
        "        'accuracy': accuracy,\n",
        "        'macro_f1': macro_f1,\n",
        "        'micro_f1': micro_f1,\n",
        "        'auroc_macro': auroc_macro,\n",
        "        'auroc_micro': auroc_micro,\n",
        "        'per_class_precision': precision,\n",
        "        'per_class_recall': recall,\n",
        "        'per_class_f1': f1,\n",
        "        'per_class_support': support\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "print(\"\\nâœ… Evaluation function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Xe7zNRWGYB"
      },
      "source": [
        "## ðŸ©º Cell 14: STAGE 1 - Binary Classifier (Healthy vs Diseased)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "435vRIOhWGYB",
        "outputId": "74043592-9834-4d3a-ef3f-5f51752b741d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ðŸ©º STAGE 1: BINARY CLASSIFICATION (Healthy vs Diseased)\n",
            "================================================================================\n",
            "\n",
            "âœ“ Created binary dataloaders\n",
            "   Batch size: 64\n",
            "âœ“ Using Focal Loss (alpha=0.25, gamma=2.0)\n",
            "âœ“ Optimizer: AdamW (lr=0.001, wd=5e-05)\n",
            "âœ“ Using CosineAnnealingWarmRestarts scheduler\n",
            "\n",
            "ðŸ‹ï¸  Training Stage 1 (Binary)...\n",
            "============================================================\n",
            "Epoch [  1/100] Train Loss: 0.0423 Acc: 0.6848 | Val Loss: 0.0299 Acc: 0.7797 F1: 0.7788 | LR: 0.000200\n",
            "Epoch [  5/100] Train Loss: 0.0218 Acc: 0.8498 | Val Loss: 0.0281 Acc: 0.8162 F1: 0.8161 | LR: 0.001000\n",
            "Epoch [ 10/100] Train Loss: 0.0065 Acc: 0.9627 | Val Loss: 0.0364 Acc: 0.8421 F1: 0.8420 | LR: 0.000024\n",
            "Epoch [ 15/100] Train Loss: 0.0096 Acc: 0.9453 | Val Loss: 0.0349 Acc: 0.8354 F1: 0.8353 | LR: 0.000905\n",
            "Epoch [ 20/100] Train Loss: 0.0048 Acc: 0.9738 | Val Loss: 0.0521 Acc: 0.8345 F1: 0.8345 | LR: 0.000578\n",
            "Epoch [ 25/100] Train Loss: 0.0019 Acc: 0.9895 | Val Loss: 0.0648 Acc: 0.8390 F1: 0.8389 | LR: 0.000206\n",
            "\n",
            "â¹ï¸  Early stopping at epoch 25\n",
            "============================================================\n",
            "âœ… Training complete!\n",
            "   Best validation loss: 0.0364\n",
            "   Best validation F1: 0.8420\n",
            "\n",
            "ðŸ“Š Evaluating Stage 1 (Binary)...\n",
            "============================================================\n",
            "\n",
            "ðŸŽ¯ Overall Metrics:\n",
            "   Accuracy:  0.8301\n",
            "   Macro F1:  0.8300\n",
            "   Micro F1:  0.8301\n",
            "   AUROC (Macro): 0.9122\n",
            "   AUROC (Micro): 0.9122\n",
            "\n",
            "ðŸ“Š Per-Class Metrics:\n",
            "Class                 Precision     Recall         F1    Support\n",
            "------------------------------------------------------------\n",
            "Healthy                  0.8195     0.8466     0.8328       1121\n",
            "Diseased                 0.8413     0.8136     0.8272       1121\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ©º STAGE 1: BINARY CLASSIFICATION (Healthy vs Diseased)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create binary dataloaders\n",
        "train_loader_binary, val_loader_binary, test_loader_binary = create_dataloaders(\n",
        "    data_splits,\n",
        "    CONFIG['batch_size'],\n",
        "    sampler=None,  # No weighted sampler\n",
        "    binary=True\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Created binary dataloaders\")\n",
        "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
        "\n",
        "# Initialize Stage 1 model\n",
        "model_stage1 = DiseaseClassifier(\n",
        "    input_dim=input_dim,\n",
        "    num_classes=2,\n",
        "    hidden_dims=CONFIG['hidden_dims'],\n",
        "    dropout=CONFIG['dropout'],\n",
        "    use_batch_norm=CONFIG['use_batch_norm']\n",
        ")\n",
        "\n",
        "# ========== UPDATED: Loss function ==========\n",
        "if CONFIG['use_focal_loss']:\n",
        "    criterion_stage1 = FocalLoss(\n",
        "        alpha=CONFIG['focal_alpha'],\n",
        "        gamma=CONFIG['focal_gamma']\n",
        "    )\n",
        "    print(f\"âœ“ Using Focal Loss (alpha={CONFIG['focal_alpha']}, gamma={CONFIG['focal_gamma']})\")\n",
        "elif CONFIG['use_class_weights']:\n",
        "    criterion_stage1 = nn.CrossEntropyLoss(weight=class_weights_binary.to(device))\n",
        "    print(f\"âœ“ Using weighted CrossEntropyLoss\")\n",
        "else:\n",
        "    criterion_stage1 = nn.CrossEntropyLoss()\n",
        "    print(f\"âœ“ Using CrossEntropyLoss\")\n",
        "\n",
        "# ========== UPDATED: Optimizer (AdamW) ==========\n",
        "optimizer_stage1 = optim.AdamW(  # Changed from Adam to AdamW\n",
        "    model_stage1.parameters(),\n",
        "    lr=CONFIG['learning_rate'],\n",
        "    weight_decay=CONFIG['weight_decay']\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Optimizer: AdamW (lr={CONFIG['learning_rate']}, wd={CONFIG['weight_decay']})\")\n",
        "\n",
        "# ========== NEW: Learning Rate Scheduler ==========\n",
        "if CONFIG['use_lr_scheduler']:\n",
        "    if CONFIG['scheduler_type'] == 'cosine':\n",
        "        scheduler_stage1 = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer_stage1,\n",
        "            T_0=10,  # Restart every 10 epochs\n",
        "            T_mult=2  # Double the restart interval each time\n",
        "        )\n",
        "        print(f\"âœ“ Using CosineAnnealingWarmRestarts scheduler\")\n",
        "    elif CONFIG['scheduler_type'] == 'plateau':\n",
        "        scheduler_stage1 = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer_stage1,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=True\n",
        "        )\n",
        "        print(f\"âœ“ Using ReduceLROnPlateau scheduler\")\n",
        "else:\n",
        "    scheduler_stage1 = None\n",
        "    print(f\"âœ“ No LR scheduler\")\n",
        "\n",
        "# Train Stage 1\n",
        "model_stage1, history_stage1 = train_model(\n",
        "    model_stage1,\n",
        "    train_loader_binary,\n",
        "    val_loader_binary,\n",
        "    criterion_stage1,\n",
        "    optimizer_stage1,\n",
        "    CONFIG['num_epochs'],\n",
        "    CONFIG['patience'],\n",
        "    device,\n",
        "    scheduler=scheduler_stage1,\n",
        "    warmup_epochs=CONFIG['lr_warmup_epochs'],\n",
        "    model_name=\"Stage 1 (Binary)\"\n",
        ")\n",
        "\n",
        "# Evaluate Stage 1\n",
        "binary_class_names = ['Healthy', 'Diseased']\n",
        "results_stage1 = evaluate_model(\n",
        "    model_stage1,\n",
        "    test_loader_binary,\n",
        "    device,\n",
        "    binary_class_names,\n",
        "    stage_name=\"Stage 1 (Binary)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZVpz9k9WGYC"
      },
      "source": [
        "## ðŸ§« Cell 16: STAGE 2 - Multi-Class Disease Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQy8QHldWGYC",
        "outputId": "5b6b9c86-723f-439c-e3fb-f00af6728cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ðŸ§« STAGE 2: MULTI-CLASS DISEASE CLASSIFICATION\n",
            "================================================================================\n",
            "\n",
            "ðŸ” Filtering Training Data: DISEASED SAMPLES ONLY\n",
            "============================================================\n",
            "Original - Train: 10460, Val: 2242, Test: 2242\n",
            "Diseased - Train: 5230, Val: 1121, Test: 1121\n",
            "\n",
            "ðŸ“‹ Disease Label Mapping (10 diseases, no Healthy):\n",
            "   Stage 2 index 0 â†’ COVID19\n",
            "   Stage 2 index 1 â†’ Colorectal_Neoplasms\n",
            "   Stage 2 index 2 â†’ Crohn_s\n",
            "   Stage 2 index 3 â†’ Diabetes\n",
            "   Stage 2 index 4 â†’ IBS\n",
            "   Stage 2 index 5 â†’ KidneyFailure\n",
            "   Stage 2 index 6 â†’ Parkinsons\n",
            "   Stage 2 index 7 â†’ Ulcerative_Colitis\n",
            "   Stage 2 index 8 â†’ cysticfibrosis\n",
            "   Stage 2 index 9 â†’ nafld\n",
            "============================================================\n",
            "\n",
            "âœ… Stage 2 will classify these 10 diseases:\n",
            "   0: COVID19\n",
            "   1: Colorectal_Neoplasms\n",
            "   2: Crohn_s\n",
            "   3: Diabetes\n",
            "   4: IBS\n",
            "   5: KidneyFailure\n",
            "   6: Parkinsons\n",
            "   7: Ulcerative_Colitis\n",
            "   8: cysticfibrosis\n",
            "   9: nafld\n",
            "============================================================\n",
            "\n",
            "âš–ï¸ Class weights for Stage 2 (10 diseases):\n",
            "   COVID19                  : 0.4121\n",
            "   Colorectal_Neoplasms     : 0.9423\n",
            "   Crohn_s                  : 0.5246\n",
            "   Diabetes                 : 2.2939\n",
            "   IBS                      : 1.9736\n",
            "   KidneyFailure            : 2.1523\n",
            "   Parkinsons               : 1.3584\n",
            "   Ulcerative_Colitis       : 0.8702\n",
            "   cysticfibrosis           : 1.5292\n",
            "   nafld                    : 1.5159\n",
            "\n",
            "âœ“ Created multi-class dataloaders (diseased samples only)\n",
            "   Batch size: 64\n",
            "   Number of classes: 10\n",
            "âœ“ Using Focal Loss for 10 disease classes\n",
            "âœ“ Optimizer: AdamW (lr=0.001, wd=5e-05)\n",
            "âœ“ Using CosineAnnealingWarmRestarts scheduler\n",
            "\n",
            "ðŸ‹ï¸  Training Stage 2 (Multi-Class)...\n",
            "============================================================\n",
            "Epoch [  1/100] Train Loss: 0.3124 Acc: 0.4281 | Val Loss: 0.1906 Acc: 0.6441 F1: 0.6033 | LR: 0.000200\n",
            "Epoch [  5/100] Train Loss: 0.0500 Acc: 0.8635 | Val Loss: 0.0914 Acc: 0.7788 F1: 0.7677 | LR: 0.001000\n",
            "Epoch [ 10/100] Train Loss: 0.0110 Acc: 0.9667 | Val Loss: 0.0892 Acc: 0.8127 F1: 0.8058 | LR: 0.000024\n",
            "Epoch [ 15/100] Train Loss: 0.0180 Acc: 0.9457 | Val Loss: 0.1081 Acc: 0.8002 F1: 0.7913 | LR: 0.000905\n",
            "Epoch [ 20/100] Train Loss: 0.0065 Acc: 0.9795 | Val Loss: 0.1153 Acc: 0.8064 F1: 0.7992 | LR: 0.000578\n",
            "Epoch [ 25/100] Train Loss: 0.0025 Acc: 0.9916 | Val Loss: 0.1136 Acc: 0.8216 F1: 0.8095 | LR: 0.000206\n",
            "Epoch [ 30/100] Train Loss: 0.0017 Acc: 0.9952 | Val Loss: 0.1163 Acc: 0.8198 F1: 0.8061 | LR: 0.000006\n",
            "\n",
            "â¹ï¸  Early stopping at epoch 34\n",
            "============================================================\n",
            "âœ… Training complete!\n",
            "   Best validation loss: 0.1113\n",
            "   Best validation F1: 0.8141\n",
            "\n",
            "ðŸ“Š Evaluating Stage 2 (Multi-Class - Diseases Only)...\n",
            "============================================================\n",
            "\n",
            "ðŸŽ¯ Overall Metrics:\n",
            "   Accuracy:  0.7690\n",
            "   Macro F1:  0.7601\n",
            "   Micro F1:  0.7690\n",
            "   AUROC (Macro): 0.9657\n",
            "   AUROC (Micro): 0.9709\n",
            "\n",
            "ðŸ“Š Per-Class Metrics:\n",
            "Class                 Precision     Recall         F1    Support\n",
            "------------------------------------------------------------\n",
            "COVID19                  0.8927     0.8566     0.8743        272\n",
            "Colorectal_Neoplasms     0.6267     0.7899     0.6989        119\n",
            "Crohn_s                  0.7143     0.7477     0.7306        214\n",
            "Diabetes                 0.6000     0.7347     0.6606         49\n",
            "IBS                      0.8644     0.8947     0.8793         57\n",
            "KidneyFailure            0.8000     0.7692     0.7843         52\n",
            "Parkinsons               0.7377     0.5488     0.6294         82\n",
            "Ulcerative_Colitis       0.7304     0.6512     0.6885        129\n",
            "cysticfibrosis           0.9104     0.8356     0.8714         73\n",
            "nafld                    0.7838     0.7838     0.7838         74\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ§« STAGE 2: MULTI-CLASS DISEASE CLASSIFICATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nðŸ” Filtering Training Data: DISEASED SAMPLES ONLY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Filter out Healthy samples (class 4) from all splits\n",
        "healthy_idx = 4\n",
        "\n",
        "# Training set - diseased only\n",
        "diseased_train_mask = (data_splits['y_train'] != healthy_idx)\n",
        "X_train_diseased = data_splits['X_train'][diseased_train_mask]\n",
        "y_train_diseased_original = data_splits['y_train'][diseased_train_mask]\n",
        "\n",
        "# Validation set - diseased only\n",
        "diseased_val_mask = (data_splits['y_val'] != healthy_idx)\n",
        "X_val_diseased = data_splits['X_val'][diseased_val_mask]\n",
        "y_val_diseased_original = data_splits['y_val'][diseased_val_mask]\n",
        "\n",
        "# Test set - diseased only\n",
        "diseased_test_mask = (data_splits['y_test'] != healthy_idx)\n",
        "X_test_diseased = data_splits['X_test'][diseased_test_mask]\n",
        "y_test_diseased_original = data_splits['y_test'][diseased_test_mask]\n",
        "\n",
        "print(f\"Original - Train: {len(data_splits['y_train'])}, Val: {len(data_splits['y_val'])}, Test: {len(data_splits['y_test'])}\")\n",
        "print(f\"Diseased - Train: {len(X_train_diseased)}, Val: {len(X_val_diseased)}, Test: {len(X_test_diseased)}\")\n",
        "\n",
        "# Create mapping from original labels to Stage 2 labels (0-9 for 10 diseases)\n",
        "original_disease_classes = [i for i in range(len(label_encoder.classes_)) if i != healthy_idx]\n",
        "disease_idx_to_name = {}\n",
        "disease_label_mapping = {}\n",
        "\n",
        "for new_idx, original_idx in enumerate(original_disease_classes):\n",
        "    disease_name = label_encoder.inverse_transform([original_idx])[0]\n",
        "    disease_idx_to_name[new_idx] = disease_name\n",
        "    disease_label_mapping[original_idx] = new_idx\n",
        "\n",
        "print(f\"\\nðŸ“‹ Disease Label Mapping (10 diseases, no Healthy):\")\n",
        "for new_idx, disease_name in disease_idx_to_name.items():\n",
        "    print(f\"   Stage 2 index {new_idx} â†’ {disease_name}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Remap labels for Stage 2\n",
        "y_train_stage2 = np.array([disease_label_mapping[y] for y in y_train_diseased_original])\n",
        "y_val_stage2 = np.array([disease_label_mapping[y] for y in y_val_diseased_original])\n",
        "y_test_stage2 = np.array([disease_label_mapping[y] for y in y_test_diseased_original])\n",
        "\n",
        "# Create class names for Stage 2 (10 diseases)\n",
        "multiclass_names_stage2 = [disease_idx_to_name[i] for i in range(len(disease_idx_to_name))]\n",
        "\n",
        "print(f\"\\nâœ… Stage 2 will classify these {len(multiclass_names_stage2)} diseases:\")\n",
        "for i, name in enumerate(multiclass_names_stage2):\n",
        "    print(f\"   {i}: {name}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Compute class weights for Stage 2 (10 diseases only)\n",
        "class_weights_stage2 = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train_stage2),\n",
        "    y=y_train_stage2\n",
        ")\n",
        "class_weights_stage2 = torch.FloatTensor(class_weights_stage2)\n",
        "\n",
        "print(f\"\\nâš–ï¸ Class weights for Stage 2 (10 diseases):\")\n",
        "for i, weight in enumerate(class_weights_stage2):\n",
        "    print(f\"   {multiclass_names_stage2[i]:25s}: {weight:.4f}\")\n",
        "\n",
        "# Create datasets for Stage 2\n",
        "train_dataset_stage2 = MicrobiomeDataset(X_train_diseased, y_train_stage2, binary_labels=False)\n",
        "val_dataset_stage2 = MicrobiomeDataset(X_val_diseased, y_val_stage2, binary_labels=False)\n",
        "test_dataset_stage2 = MicrobiomeDataset(X_test_diseased, y_test_stage2, binary_labels=False)\n",
        "\n",
        "# Create dataloaders for Stage 2\n",
        "train_loader_stage2 = DataLoader(\n",
        "    train_dataset_stage2,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=True\n",
        ")\n",
        "val_loader_stage2 = DataLoader(\n",
        "    val_dataset_stage2,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=False\n",
        ")\n",
        "test_loader_stage2 = DataLoader(\n",
        "    test_dataset_stage2,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Created multi-class dataloaders (diseased samples only)\")\n",
        "print(f\"   Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"   Number of classes: {len(multiclass_names_stage2)}\")\n",
        "\n",
        "# Initialize Stage 2 model\n",
        "model_stage2 = DiseaseClassifier(\n",
        "    input_dim=input_dim,\n",
        "    num_classes=len(multiclass_names_stage2),  # 10 diseases\n",
        "    hidden_dims=CONFIG['hidden_dims'],\n",
        "    dropout=CONFIG['dropout'],\n",
        "    use_batch_norm=CONFIG['use_batch_norm']\n",
        ")\n",
        "\n",
        "# ========== UPDATED: Loss function with Focal Loss ==========\n",
        "if CONFIG['use_focal_loss']:\n",
        "    criterion_stage2 = FocalLoss(\n",
        "        alpha=CONFIG['focal_alpha'],\n",
        "        gamma=CONFIG['focal_gamma']\n",
        "    )\n",
        "    print(f\"âœ“ Using Focal Loss for {len(multiclass_names_stage2)} disease classes\")\n",
        "elif CONFIG['use_class_weights']:\n",
        "    criterion_stage2 = nn.CrossEntropyLoss(weight=class_weights_stage2.to(device))\n",
        "    print(f\"âœ“ Using weighted CrossEntropyLoss for {len(multiclass_names_stage2)} disease classes\")\n",
        "else:\n",
        "    criterion_stage2 = nn.CrossEntropyLoss()\n",
        "    print(f\"âœ“ Using CrossEntropyLoss\")\n",
        "\n",
        "# ========== UPDATED: Optimizer (AdamW) ==========\n",
        "optimizer_stage2 = optim.AdamW(\n",
        "    model_stage2.parameters(),\n",
        "    lr=CONFIG['learning_rate'],\n",
        "    weight_decay=CONFIG['weight_decay']\n",
        ")\n",
        "\n",
        "print(f\"âœ“ Optimizer: AdamW (lr={CONFIG['learning_rate']}, wd={CONFIG['weight_decay']})\")\n",
        "\n",
        "# ========== NEW: Learning Rate Scheduler ==========\n",
        "if CONFIG['use_lr_scheduler']:\n",
        "    if CONFIG['scheduler_type'] == 'cosine':\n",
        "        scheduler_stage2 = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer_stage2,\n",
        "            T_0=10,\n",
        "            T_mult=2\n",
        "        )\n",
        "        print(f\"âœ“ Using CosineAnnealingWarmRestarts scheduler\")\n",
        "    elif CONFIG['scheduler_type'] == 'plateau':\n",
        "        scheduler_stage2 = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer_stage2,\n",
        "            mode='min',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            verbose=True\n",
        "        )\n",
        "        print(f\"âœ“ Using ReduceLROnPlateau scheduler\")\n",
        "else:\n",
        "    scheduler_stage2 = None\n",
        "    print(f\"âœ“ No LR scheduler\")\n",
        "\n",
        "# Train Stage 2\n",
        "model_stage2, history_stage2 = train_model(\n",
        "    model_stage2,\n",
        "    train_loader_stage2,\n",
        "    val_loader_stage2,\n",
        "    criterion_stage2,\n",
        "    optimizer_stage2,\n",
        "    CONFIG['num_epochs'],\n",
        "    CONFIG['patience'],\n",
        "    device,\n",
        "    scheduler=scheduler_stage2,\n",
        "    warmup_epochs=CONFIG['lr_warmup_epochs'],\n",
        "    model_name=\"Stage 2 (Multi-Class)\"\n",
        ")\n",
        "\n",
        "# Evaluate Stage 2\n",
        "results_stage2 = evaluate_model(\n",
        "    model_stage2,\n",
        "    test_loader_stage2,\n",
        "    device,\n",
        "    multiclass_names_stage2,\n",
        "    stage_name=\"Stage 2 (Multi-Class - Diseases Only)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13gxf7E9WGYC"
      },
      "source": [
        "## ðŸ”— Cell 18: Combined Two-Stage Inference Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOtWfO2HWGYC",
        "outputId": "147454cf-b4d9-40d6-e061-d048472ad7ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Updated two-stage prediction function with soft thresholding\n"
          ]
        }
      ],
      "source": [
        "def predict_with_risk(sample, model_stage1, model_stage2, scaler,\n",
        "                     feature_selector, label_encoder, disease_idx_to_name,\n",
        "                     disease_label_mapping, device,\n",
        "                     threshold_low=0.3, threshold_high=0.7):\n",
        "    \"\"\"\n",
        "    Two-stage prediction with SOFT THRESHOLDING and uncertainty handling.\n",
        "\n",
        "    Args:\n",
        "        threshold_low: Below this, classify as Healthy\n",
        "        threshold_high: Above this, classify as Diseased\n",
        "        Between low and high: Mark as Uncertain\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with prediction, confidence, and risk scores\n",
        "    \"\"\"\n",
        "    model_stage1.eval()\n",
        "    model_stage2.eval()\n",
        "\n",
        "    # ========== FIXED: Correct preprocessing order ==========\n",
        "    # 1. Feature selection first (2378 â†’ 500 features)\n",
        "    sample_selected = feature_selector.transform(sample.reshape(1, -1))\n",
        "    # 2. Then scaling (500 â†’ 500 features)\n",
        "    sample_scaled = scaler.transform(sample_selected)\n",
        "    # 3. Convert to tensor\n",
        "    sample_tensor = torch.FloatTensor(sample_scaled).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Stage 1: Healthy vs Diseased\n",
        "        output_stage1 = model_stage1(sample_tensor)\n",
        "        probs_stage1 = F.softmax(output_stage1, dim=1).cpu().numpy()[0]\n",
        "\n",
        "        healthy_prob = probs_stage1[0]\n",
        "        disease_prob = probs_stage1[1]\n",
        "\n",
        "        # Stage 2: Multi-class probabilities\n",
        "        output_stage2 = model_stage2(sample_tensor)\n",
        "        probs_stage2 = F.softmax(output_stage2, dim=1).cpu().numpy()[0]\n",
        "\n",
        "        # ========== Soft thresholding logic ==========\n",
        "        if healthy_prob > threshold_high:\n",
        "            # High confidence it's healthy\n",
        "            predicted_name = \"Healthy\"\n",
        "            confidence = \"High\"\n",
        "        elif disease_prob > threshold_high:\n",
        "            # High confidence it's diseased\n",
        "            stage2_predicted_idx = np.argmax(probs_stage2)\n",
        "            predicted_name = disease_idx_to_name[stage2_predicted_idx]\n",
        "            confidence = \"High\"\n",
        "        elif healthy_prob > threshold_low:\n",
        "            # Leaning healthy but uncertain\n",
        "            predicted_name = \"Healthy\"\n",
        "            confidence = \"Low\"\n",
        "        elif disease_prob > threshold_low:\n",
        "            # Leaning diseased but uncertain\n",
        "            stage2_predicted_idx = np.argmax(probs_stage2)\n",
        "            predicted_name = disease_idx_to_name[stage2_predicted_idx]\n",
        "            confidence = \"Low\"\n",
        "        else:\n",
        "            # Very uncertain - defer to highest Stage 2 probability\n",
        "            stage2_predicted_idx = np.argmax(probs_stage2)\n",
        "            predicted_name = disease_idx_to_name[stage2_predicted_idx]\n",
        "            confidence = \"Uncertain\"\n",
        "\n",
        "        # Compute risk scores\n",
        "        healthy_idx = np.where(label_encoder.classes_ == \"Healthy\")[0][0]\n",
        "        risk_scores = np.zeros(len(label_encoder.classes_))\n",
        "        risk_scores[healthy_idx] = healthy_prob\n",
        "\n",
        "        for stage2_idx in range(len(disease_idx_to_name)):\n",
        "            disease_name = disease_idx_to_name[stage2_idx]\n",
        "            original_idx = np.where(label_encoder.classes_ == disease_name)[0][0]\n",
        "            risk_scores[original_idx] = disease_prob * probs_stage2[stage2_idx]\n",
        "\n",
        "        risk_scores = risk_scores / risk_scores.sum()\n",
        "\n",
        "    # Build results\n",
        "    per_class_probs = {}\n",
        "    for i, class_name in enumerate(label_encoder.classes_):\n",
        "        per_class_probs[class_name] = float(risk_scores[i])\n",
        "\n",
        "    results_dict = {\n",
        "        \"Predicted\": predicted_name,\n",
        "        \"Confidence\": confidence,\n",
        "        \"Healthy_Prob\": float(healthy_prob),\n",
        "        \"Disease_Prob\": float(disease_prob),\n",
        "        \"Risk_Weighted_Probabilities\": per_class_probs\n",
        "    }\n",
        "\n",
        "    return results_dict\n",
        "\n",
        "\n",
        "print(\"\\nâœ… Updated two-stage prediction function with soft thresholding\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auFE-NX2WGYC"
      },
      "source": [
        "## ðŸ§ª Cell 19: Test Two-Stage Pipeline on Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvjAAdj1WGYD",
        "outputId": "091e80fd-140d-4ecc-d2f4-76162410dbf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ§ª Testing Two-Stage Pipeline\n",
            "============================================================\n",
            "\n",
            "ðŸ“‹ Sample 1 (True Label: Healthy)\n",
            "------------------------------------------------------------\n",
            "{\n",
            "  \"Predicted\": \"Healthy\",\n",
            "  \"Confidence\": \"High\",\n",
            "  \"Healthy_Prob\": 0.9762320518493652,\n",
            "  \"Disease_Prob\": 0.023767979815602303,\n",
            "  \"Risk_Weighted_Probabilities\": {\n",
            "    \"COVID19\": 0.0003340649020728591,\n",
            "    \"Colorectal_Neoplasms\": 2.4794183563001198e-05,\n",
            "    \"Crohn_s\": 0.013540181829919685,\n",
            "    \"Diabetes\": 0.0003495772141066997,\n",
            "    \"Healthy\": 0.9762320206493376,\n",
            "    \"IBS\": 5.6702085209952863e-05,\n",
            "    \"KidneyFailure\": 1.2303140072315416e-05,\n",
            "    \"Parkinsons\": 0.00026910201135593097,\n",
            "    \"Ulcerative_Colitis\": 0.009113701700466229,\n",
            "    \"cysticfibrosis\": 4.345771091984356e-05,\n",
            "    \"nafld\": 2.4094572975915713e-05\n",
            "  }\n",
            "}\n",
            "\n",
            "âœ“ Prediction: Correct\n",
            "   Confidence: High\n",
            "\n",
            "ðŸ“‹ Sample 2 (True Label: COVID19)\n",
            "------------------------------------------------------------\n",
            "{\n",
            "  \"Predicted\": \"COVID19\",\n",
            "  \"Confidence\": \"High\",\n",
            "  \"Healthy_Prob\": 0.014937133528292179,\n",
            "  \"Disease_Prob\": 0.985062837600708,\n",
            "  \"Risk_Weighted_Probabilities\": {\n",
            "    \"COVID19\": 0.8646217219693078,\n",
            "    \"Colorectal_Neoplasms\": 0.01717760273539543,\n",
            "    \"Crohn_s\": 0.0670745763231534,\n",
            "    \"Diabetes\": 0.0002330252235415877,\n",
            "    \"Healthy\": 0.014937133927589675,\n",
            "    \"IBS\": 0.0022716280680176097,\n",
            "    \"KidneyFailure\": 0.0007188409991869545,\n",
            "    \"Parkinsons\": 0.0014771428354071598,\n",
            "    \"Ulcerative_Colitis\": 0.011387462100811069,\n",
            "    \"cysticfibrosis\": 0.017512737310184434,\n",
            "    \"nafld\": 0.0025881285074049382\n",
            "  }\n",
            "}\n",
            "\n",
            "âœ“ Prediction: Correct\n",
            "   Confidence: High\n",
            "\n",
            "ðŸ“‹ Sample 3 (True Label: cysticfibrosis)\n",
            "------------------------------------------------------------\n",
            "{\n",
            "  \"Predicted\": \"cysticfibrosis\",\n",
            "  \"Confidence\": \"High\",\n",
            "  \"Healthy_Prob\": 0.00644252123311162,\n",
            "  \"Disease_Prob\": 0.9935575127601624,\n",
            "  \"Risk_Weighted_Probabilities\": {\n",
            "    \"COVID19\": 6.018776673595546e-07,\n",
            "    \"Colorectal_Neoplasms\": 4.8348817925928584e-05,\n",
            "    \"Crohn_s\": 4.573609614987475e-06,\n",
            "    \"Diabetes\": 1.8783987166236675e-06,\n",
            "    \"Healthy\": 0.0064425211723686815,\n",
            "    \"IBS\": 2.8320334699008845e-09,\n",
            "    \"KidneyFailure\": 2.0704017149790197e-07,\n",
            "    \"Parkinsons\": 0.0015358285600801118,\n",
            "    \"Ulcerative_Colitis\": 3.101660302350933e-05,\n",
            "    \"cysticfibrosis\": 0.9918462540570958,\n",
            "    \"nafld\": 8.876703130206845e-05\n",
            "  }\n",
            "}\n",
            "\n",
            "âœ“ Prediction: Correct\n",
            "   Confidence: High\n",
            "\n",
            "ðŸ“‹ Sample 4 (True Label: Healthy)\n",
            "------------------------------------------------------------\n",
            "{\n",
            "  \"Predicted\": \"Healthy\",\n",
            "  \"Confidence\": \"High\",\n",
            "  \"Healthy_Prob\": 0.8827105164527893,\n",
            "  \"Disease_Prob\": 0.11728951334953308,\n",
            "  \"Risk_Weighted_Probabilities\": {\n",
            "    \"COVID19\": 0.014360844798688582,\n",
            "    \"Colorectal_Neoplasms\": 0.030917206154204915,\n",
            "    \"Crohn_s\": 0.03999785625554763,\n",
            "    \"Diabetes\": 0.00018223961155845871,\n",
            "    \"Healthy\": 0.882710498610906,\n",
            "    \"IBS\": 0.0014904368134707805,\n",
            "    \"KidneyFailure\": 7.056671750590364e-05,\n",
            "    \"Parkinsons\": 0.0007256102252304466,\n",
            "    \"Ulcerative_Colitis\": 0.029146838344095764,\n",
            "    \"cysticfibrosis\": 0.00012426194483705562,\n",
            "    \"nafld\": 0.00027364052395443015\n",
            "  }\n",
            "}\n",
            "\n",
            "âœ“ Prediction: Correct\n",
            "   Confidence: High\n",
            "\n",
            "ðŸ“‹ Sample 5 (True Label: Healthy)\n",
            "------------------------------------------------------------\n",
            "{\n",
            "  \"Predicted\": \"Healthy\",\n",
            "  \"Confidence\": \"High\",\n",
            "  \"Healthy_Prob\": 0.9962020516395569,\n",
            "  \"Disease_Prob\": 0.003797943005338311,\n",
            "  \"Risk_Weighted_Probabilities\": {\n",
            "    \"COVID19\": 7.08126302883907e-06,\n",
            "    \"Colorectal_Neoplasms\": 0.001184395636860108,\n",
            "    \"Crohn_s\": 4.179254777537733e-05,\n",
            "    \"Diabetes\": 0.00236612210320781,\n",
            "    \"Healthy\": 0.996202056699793,\n",
            "    \"IBS\": 2.1560021078050994e-07,\n",
            "    \"KidneyFailure\": 6.611301648194167e-06,\n",
            "    \"Parkinsons\": 0.00016773807959618095,\n",
            "    \"Ulcerative_Colitis\": 2.6469376869346404e-06,\n",
            "    \"cysticfibrosis\": 7.819883802470987e-07,\n",
            "    \"nafld\": 2.055784181249287e-05\n",
            "  }\n",
            "}\n",
            "\n",
            "âœ“ Prediction: Correct\n",
            "   Confidence: High\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nðŸ§ª Testing Two-Stage Pipeline\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test on a few random samples from test set\n",
        "num_samples_to_test = 5\n",
        "\n",
        "# âœ… FIXED: X_raw and y are now aligned, so we can safely sample\n",
        "test_indices = np.random.choice(len(X_raw), num_samples_to_test, replace=False)\n",
        "\n",
        "for i, idx in enumerate(test_indices, 1):\n",
        "    sample = X_raw[idx]\n",
        "    true_label = y[idx]\n",
        "    true_name = label_encoder.inverse_transform([true_label])[0]\n",
        "\n",
        "    print(f\"\\nðŸ“‹ Sample {i} (True Label: {true_name})\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    result = predict_with_risk(\n",
        "        sample,\n",
        "        model_stage1,\n",
        "        model_stage2,\n",
        "        scaler,\n",
        "        feature_selector,\n",
        "        label_encoder,\n",
        "        disease_idx_to_name,\n",
        "        disease_label_mapping,\n",
        "        device,\n",
        "        threshold_low=CONFIG['disease_threshold_low'],    # UPDATED\n",
        "        threshold_high=CONFIG['disease_threshold_high']   # UPDATED\n",
        "    )\n",
        "\n",
        "    print(json.dumps(result, indent=2))\n",
        "\n",
        "    # Check if prediction is correct\n",
        "    correct = \"âœ“\" if result['Predicted'] == true_name else \"âœ—\"\n",
        "    print(f\"\\n{correct} Prediction: {'Correct' if correct == 'âœ“' else 'Incorrect'}\")\n",
        "\n",
        "    # NEW: Show confidence level\n",
        "    print(f\"   Confidence: {result['Confidence']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdyeYlXpWGYE"
      },
      "source": [
        "## ðŸŽ¯ Cell 20: Evaluate Full Two-Stage Pipeline on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfz55KLYWGYE",
        "outputId": "1532f57b-fe44-4ad1-fb01-ad51f8d7bad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ¯ Evaluating Two-Stage Pipeline on Test Set\n",
            "============================================================\n",
            "\n",
            "ðŸŽ¯ Two-Stage Pipeline Performance:\n",
            "   Accuracy:      0.7435\n",
            "   Macro F1:      0.6295\n",
            "   Micro F1:      0.7435\n",
            "   AUROC (Macro): 0.9527\n",
            "   AUROC (Micro): 0.9705\n",
            "\n",
            "ðŸ“Š Confidence Distribution:\n",
            "   High      : 1911 ( 85.2%)\n",
            "   Low       :  331 ( 14.8%)\n",
            "   Uncertain :    0 (  0.0%)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "def evaluate_two_stage_pipeline(X_test, y_test, model_stage1, model_stage2,\n",
        "                                scaler, feature_selector, label_encoder,\n",
        "                                disease_idx_to_name, disease_label_mapping,\n",
        "                                threshold_low, threshold_high, device):  # UPDATED\n",
        "    \"\"\"\n",
        "    Evaluate the complete two-stage pipeline on test set.\n",
        "    \"\"\"\n",
        "    print(\"\\nðŸŽ¯ Evaluating Two-Stage Pipeline on Test Set\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    all_predictions = []\n",
        "    all_risk_scores = []\n",
        "    all_confidences = []  # NEW: Track confidence\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        result = predict_with_risk(\n",
        "            X_test[i],\n",
        "            model_stage1,\n",
        "            model_stage2,\n",
        "            scaler,\n",
        "            feature_selector,\n",
        "            label_encoder,\n",
        "            disease_idx_to_name,\n",
        "            disease_label_mapping,\n",
        "            device,\n",
        "            threshold_low=threshold_low,    # NEW\n",
        "            threshold_high=threshold_high   # NEW\n",
        "        )\n",
        "\n",
        "        predicted_class = label_encoder.transform([result['Predicted']])[0]\n",
        "        all_predictions.append(predicted_class)\n",
        "        all_confidences.append(result['Confidence'])  # NEW\n",
        "\n",
        "        risk_vector = [result['Risk_Weighted_Probabilities'][name]\n",
        "                      for name in label_encoder.classes_]\n",
        "        all_risk_scores.append(risk_vector)\n",
        "\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    all_risk_scores = np.array(all_risk_scores)\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(y_test, all_predictions)\n",
        "    macro_f1 = f1_score(y_test, all_predictions, average='macro')\n",
        "    micro_f1 = f1_score(y_test, all_predictions, average='micro')\n",
        "\n",
        "    # AUROC\n",
        "    auroc_macro = roc_auc_score(y_test, all_risk_scores, average='macro', multi_class='ovr')\n",
        "    auroc_micro = roc_auc_score(y_test, all_risk_scores, average='micro', multi_class='ovr')\n",
        "\n",
        "    print(f\"\\nðŸŽ¯ Two-Stage Pipeline Performance:\")\n",
        "    print(f\"   Accuracy:      {accuracy:.4f}\")\n",
        "    print(f\"   Macro F1:      {macro_f1:.4f}\")\n",
        "    print(f\"   Micro F1:      {micro_f1:.4f}\")\n",
        "    print(f\"   AUROC (Macro): {auroc_macro:.4f}\")\n",
        "    print(f\"   AUROC (Micro): {auroc_micro:.4f}\")\n",
        "\n",
        "    # NEW: Report confidence distribution\n",
        "    from collections import Counter\n",
        "    conf_dist = Counter(all_confidences)\n",
        "    print(f\"\\nðŸ“Š Confidence Distribution:\")\n",
        "    for conf_level in ['High', 'Low', 'Uncertain']:\n",
        "        count = conf_dist.get(conf_level, 0)\n",
        "        pct = 100 * count / len(all_confidences)\n",
        "        print(f\"   {conf_level:10s}: {count:4d} ({pct:5.1f}%)\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    results = {\n",
        "        'predictions': all_predictions,\n",
        "        'labels': y_test,\n",
        "        'probabilities': all_risk_scores,\n",
        "        'confidences': all_confidences,  # NEW\n",
        "        'accuracy': accuracy,\n",
        "        'macro_f1': macro_f1,\n",
        "        'micro_f1': micro_f1,\n",
        "        'auroc_macro': auroc_macro,\n",
        "        'auroc_micro': auroc_micro\n",
        "    }\n",
        "\n",
        "    return results\n",
        "\n",
        "# âœ… FIXED: X_raw and y are now aligned after undersampling!\n",
        "# Create aligned splits - they're already aligned now\n",
        "raw_splits = create_train_val_test_split(\n",
        "    X_raw,  # Already undersampled and aligned with y\n",
        "    y,      # Already undersampled\n",
        "    test_size=CONFIG['test_size'],\n",
        "    val_size=CONFIG['val_size'],\n",
        "    random_state=CONFIG['random_state'],\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Evaluate complete pipeline\n",
        "# Evaluate complete pipeline with soft thresholding\n",
        "pipeline_results = evaluate_two_stage_pipeline(\n",
        "    raw_splits['X_test'],\n",
        "    raw_splits['y_test'],\n",
        "    model_stage1,\n",
        "    model_stage2,\n",
        "    scaler,\n",
        "    feature_selector,\n",
        "    label_encoder,\n",
        "    disease_idx_to_name,\n",
        "    disease_label_mapping,\n",
        "    CONFIG['disease_threshold_low'],   # NEW: Use low threshold\n",
        "    CONFIG['disease_threshold_high'],  # NEW: Use high threshold\n",
        "    device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRvYHWXjWGYE"
      },
      "source": [
        "## ðŸ’¾ Cell 22: Save Models and Artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z51XQiJWGYE"
      },
      "outputs": [],
      "source": [
        "print(\"\\nðŸ’¾ Saving models and artifacts...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create output directory\n",
        "output_dir = CONFIG['output_dir']\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save models\n",
        "torch.save(model_stage1.state_dict(), os.path.join(output_dir, 'model_stage1_binary.pth'))\n",
        "torch.save(model_stage2.state_dict(), os.path.join(output_dir, 'model_stage2_multiclass.pth'))\n",
        "print(\"âœ“ Saved PyTorch models\")\n",
        "\n",
        "# Save preprocessing artifacts\n",
        "with open(os.path.join(output_dir, 'scaler.pkl'), 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "with open(os.path.join(output_dir, 'feature_selector.pkl'), 'wb') as f:\n",
        "    pickle.dump(feature_selector, f)\n",
        "\n",
        "with open(os.path.join(output_dir, 'label_encoder.pkl'), 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"âœ“ Saved preprocessing artifacts\")\n",
        "\n",
        "# Save selected feature names\n",
        "with open(os.path.join(output_dir, 'selected_features.txt'), 'w') as f:\n",
        "    for feature in selected_features:\n",
        "        f.write(f\"{feature}\\n\")\n",
        "\n",
        "print(\"âœ“ Saved selected feature names\")\n",
        "\n",
        "# Save configuration\n",
        "with open(os.path.join(output_dir, 'config.json'), 'w') as f:\n",
        "    json.dump(CONFIG, f, indent=2)\n",
        "\n",
        "print(\"âœ“ Saved configuration\")\n",
        "\n",
        "# Save results summary\n",
        "results_summary = {\n",
        "    'stage1_binary': {\n",
        "        'accuracy': float(results_stage1['accuracy']),\n",
        "        'auroc': float(results_stage1['auroc_macro']),\n",
        "        'f1_macro': float(results_stage1['macro_f1'])\n",
        "    },\n",
        "    'stage2_multiclass': {\n",
        "        'accuracy': float(results_stage2['accuracy']),\n",
        "        'auroc': float(results_stage2['auroc_macro']),\n",
        "        'f1_macro': float(results_stage2['macro_f1'])\n",
        "    },\n",
        "    'pipeline_combined': {\n",
        "        'accuracy': float(pipeline_results['accuracy']),\n",
        "        'auroc': float(pipeline_results['auroc_macro']),\n",
        "        'f1_macro': float(pipeline_results['macro_f1'])\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(os.path.join(output_dir, 'results_summary.json'), 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2)\n",
        "\n",
        "print(\"âœ“ Saved results summary\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nâœ… All artifacts saved to: {output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SigaJ9RWWGYE"
      },
      "source": [
        "## ðŸ“‹ Cell 23: Final Summary Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwpIYNj5WGYE",
        "outputId": "130e4def-9b0b-4ed0-ff50-47ad0ccbb257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "ðŸ“‹ FINAL SUMMARY REPORT\n",
            "================================================================================\n",
            "\n",
            "ðŸŽ¯ PIPELINE OVERVIEW:\n",
            "   Total samples: 14944\n",
            "   Total features (original): 2378\n",
            "   Selected features: 500\n",
            "   Number of classes (all): 11\n",
            "   Number of diseases (Stage 2): 10\n",
            "   Train/Val/Test split: 10460/2242/2242\n",
            "\n",
            "ðŸ©º STAGE 1 (Binary - Healthy vs Diseased):\n",
            "   Accuracy:      0.8301\n",
            "   AUROC:         0.9122\n",
            "   Macro F1:      0.8300\n",
            "   Recall (Diseased): 0.8136\n",
            "\n",
            "ðŸ§« STAGE 2 (Multi-Class Disease - EXCLUDES HEALTHY):\n",
            "   Number of classes: 10 diseases\n",
            "   Accuracy:      0.7690\n",
            "   AUROC (Macro): 0.9657\n",
            "   AUROC (Micro): 0.9709\n",
            "   Macro F1:      0.7601\n",
            "   Micro F1:      0.7690\n",
            "\n",
            "ðŸ”— COMBINED TWO-STAGE PIPELINE:\n",
            "   Accuracy:      0.7435\n",
            "   AUROC (Macro): 0.9527\n",
            "   AUROC (Micro): 0.9705\n",
            "   Macro F1:      0.6295\n",
            "   Micro F1:      0.7435\n",
            "\n",
            "ðŸ“ SAVED ARTIFACTS:\n",
            "   - model_stage1_binary.pth (2 classes)\n",
            "   - model_stage2_multiclass.pth (10 disease classes)\n",
            "   - scaler.pkl\n",
            "   - feature_selector.pkl\n",
            "   - label_encoder.pkl\n",
            "   - selected_features.txt\n",
            "   - config.json\n",
            "   - results_summary.json\n",
            "\n",
            "================================================================================\n",
            "âœ… Pipeline execution complete!\n",
            "   âš ï¸  IMPORTANT: Stage 2 was trained on 10 disease classes only (Healthy excluded)\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ðŸ“‹ FINAL SUMMARY REPORT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nðŸŽ¯ PIPELINE OVERVIEW:\")\n",
        "print(f\"   Total samples: {len(y)}\")\n",
        "print(f\"   Total features (original): {len(feature_names)}\")\n",
        "print(f\"   Selected features: {len(selected_features)}\")\n",
        "print(f\"   Number of classes (all): {num_classes}\")\n",
        "print(f\"   Number of diseases (Stage 2): {len(multiclass_names_stage2)}\")\n",
        "print(f\"   Train/Val/Test split: {len(data_splits['y_train'])}/{len(data_splits['y_val'])}/{len(data_splits['y_test'])}\")\n",
        "\n",
        "print(\"\\nðŸ©º STAGE 1 (Binary - Healthy vs Diseased):\")\n",
        "print(f\"   Accuracy:      {results_stage1['accuracy']:.4f}\")\n",
        "print(f\"   AUROC:         {results_stage1['auroc_macro']:.4f}\")\n",
        "print(f\"   Macro F1:      {results_stage1['macro_f1']:.4f}\")\n",
        "print(f\"   Recall (Diseased): {results_stage1['per_class_recall'][1]:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ§« STAGE 2 (Multi-Class Disease - EXCLUDES HEALTHY):\")\n",
        "print(f\"   Number of classes: {len(multiclass_names_stage2)} diseases\")\n",
        "print(f\"   Accuracy:      {results_stage2['accuracy']:.4f}\")\n",
        "print(f\"   AUROC (Macro): {results_stage2['auroc_macro']:.4f}\")\n",
        "print(f\"   AUROC (Micro): {results_stage2['auroc_micro']:.4f}\")\n",
        "print(f\"   Macro F1:      {results_stage2['macro_f1']:.4f}\")\n",
        "print(f\"   Micro F1:      {results_stage2['micro_f1']:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ”— COMBINED TWO-STAGE PIPELINE:\")\n",
        "print(f\"   Accuracy:      {pipeline_results['accuracy']:.4f}\")\n",
        "print(f\"   AUROC (Macro): {pipeline_results['auroc_macro']:.4f}\")\n",
        "print(f\"   AUROC (Micro): {pipeline_results['auroc_micro']:.4f}\")\n",
        "print(f\"   Macro F1:      {pipeline_results['macro_f1']:.4f}\")\n",
        "print(f\"   Micro F1:      {pipeline_results['micro_f1']:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ“ SAVED ARTIFACTS:\")\n",
        "print(f\"   - model_stage1_binary.pth (2 classes)\")\n",
        "print(f\"   - model_stage2_multiclass.pth ({len(multiclass_names_stage2)} disease classes)\")\n",
        "print(f\"   - scaler.pkl\")\n",
        "print(f\"   - feature_selector.pkl\")\n",
        "print(f\"   - label_encoder.pkl\")\n",
        "print(f\"   - selected_features.txt\")\n",
        "print(f\"   - config.json\")\n",
        "print(f\"   - results_summary.json\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… Pipeline execution complete!\")\n",
        "print(f\"   âš ï¸  IMPORTANT: Stage 2 was trained on {len(multiclass_names_stage2)} disease classes only (Healthy excluded)\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}